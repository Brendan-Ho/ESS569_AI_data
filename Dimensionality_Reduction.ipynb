{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcpjKSF4EvSLVuMRbB5rui",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brendan-Ho/ESS569_AI_data/blob/main/Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "bx0xyQGG8x4L",
        "outputId": "c1308930-2fb6-4325-8b34-e8f34138f82a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nHigh Dimensionality in DNA Sequencing Data\\n\\nDNA sequencing data often poses a unique challenge due to its high dimensionality. In most Machine Learning (ML) applications involving DNA or microbial sequencing, the data's dimensionality is increased by the use of **k-mers**—short DNA subsequences of length k (e.g., 4-mer, 6-mer). This approach involves breaking down DNA sequences into overlapping substrings of fixed length, which can provide valuable insights into sequence composition. However, this also results in an enormous feature space, as each possible k-mer represents a unique feature.\\n\\nFor instance, a 6-mer has 4^6 = 4,096 possible combinations, so a dataset based on 6-mers alone may have thousands of features per sample. Additionally, k-mer counts across DNA samples can lead to **sparse data matrices**, where many counts may be zero, further complicating the analysis. \\n\\nHigh dimensionality can result in:\\n- Increased computational cost for model training and testing.\\n- Potential overfitting in ML models due to an excessive number of features relative to sample size.\\n- Difficulty visualizing and interpreting relationships among samples, as meaningful clusters may not appear in higher-dimensional spaces.\\n\\nTo manage these challenges, dimensionality reduction techniques like PCA and t-SNE can be used. PCA helps reduce the feature space while retaining as much variance as possible, useful for linear patterns. t-SNE, on the other hand, captures non-linear relationships and is excellent for visualizing clusters in a lower-dimensional space. Together, these techniques aid in data interpretation and provide insights into clustering patterns and feature relevance for downstream ML tasks.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "High Dimensionality in DNA Sequencing Data\n",
        "\n",
        "DNA sequencing data often poses a unique challenge due to its high dimensionality. In most Machine Learning (ML) applications involving DNA or microbial sequencing, the data's dimensionality is increased by the use of **k-mers**—short DNA subsequences of length k (e.g., 4-mer, 6-mer). This approach involves breaking down DNA sequences into overlapping substrings of fixed length, which can provide valuable insights into sequence composition. However, this also results in an enormous feature space, as each possible k-mer represents a unique feature.\n",
        "\n",
        "For instance, a 6-mer has 4^6 = 4,096 possible combinations, so a dataset based on 6-mers alone may have thousands of features per sample. Additionally, k-mer counts across DNA samples can lead to **sparse data matrices**, where many counts may be zero, further complicating the analysis.\n",
        "\n",
        "High dimensionality can result in:\n",
        "- Increased computational cost for model training and testing.\n",
        "- Potential overfitting in ML models due to an excessive number of features relative to sample size.\n",
        "- Difficulty visualizing and interpreting relationships among samples, as meaningful clusters may not appear in higher-dimensional spaces.\n",
        "\n",
        "To manage these challenges, dimensionality reduction techniques like PCA and t-SNE can be used. PCA helps reduce the feature space while retaining as much variance as possible, useful for linear patterns. t-SNE, on the other hand, captures non-linear relationships and is excellent for visualizing clusters in a lower-dimensional space. Together, these techniques aid in data interpretation and provide insights into clustering patterns and feature relevance for downstream ML tasks.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load your dataset\n",
        "# Example: df = pd.read_csv('path_to_your_data.csv')\n",
        "# Ensure data is scaled and cleaned before applying PCA/t-SNE\n",
        "\n",
        "# Define your features and target\n",
        "X = df.drop(columns=['target'])  # Replace 'target' with the actual label column if applicable\n",
        "y = df['target']  # Optional: only if you have a label for coloring\n",
        "\n",
        "### Dimensionality Reduction with PCA ###\n",
        "\n",
        "# Initialize PCA with desired components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot PCA results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"PCA Result (2D Projection)\")\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette=\"viridis\")  # Remove hue if no target\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n",
        "\n",
        "# Explained Variance Chart\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.title(\"Explained Variance by PCA Components\")\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.show()\n",
        "\n",
        "### Dimensionality Reduction with t-SNE ###\n",
        "\n",
        "# Initialize t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=0)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"t-SNE Result (2D Projection)\")\n",
        "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette=\"viridis\")  # Remove hue if no target\n",
        "plt.xlabel(\"t-SNE Component 1\")\n",
        "plt.ylabel(\"t-SNE Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "icLq-ozz82XQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}