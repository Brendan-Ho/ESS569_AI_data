{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protbert, CNN, LSTM Classification of Protein Sequences by Tropism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, roc_curve, auc\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and models for Protbert, CNN, and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Read your FASTA file and prepare data\n",
    "def read_fasta(file_path):\n",
    "    \"\"\"Reads a FASTA file and returns protein sequences and labels.\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        label = get_tropism_label(record.id)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "def get_tropism_label(record_id):\n",
    "    \"\"\"Assign tropism labels based on sequence ID or other logic.\"\"\"\n",
    "    if \"TrophismTypeA\" in record_id:\n",
    "        return 0\n",
    "    elif \"TrophismTypeB\" in record_id:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2  # Default case, adjust as needed\n",
    "\n",
    "# Step 2: Create a custom dataset class\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the protein sequence\n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        item = {key: encoding[key].squeeze(0) for key in encoding}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Step 3: Define the CNN and LSTM models separately\n",
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, dropout=0.5):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # Change 3 to match number of classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change to (batch_size, channels, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)  # Change to (batch_size, seq_len, channels)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, dropout=0.5):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # Change 3 to match number of classes\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = self.dropout(hn[-1])  # Use the last hidden state\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 4: Define the ProtBERT model for fine-tuning\n",
    "class ProtBERT_Model(nn.Module):\n",
    "    def __init__(self, transformer_model):\n",
    "        super(ProtBERT_Model, self).__init__()\n",
    "        self.transformer_model = transformer_model\n",
    "        self.fc = nn.Linear(768, 3)  # Change 3 to match number of classes\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_outputs = self.transformer_model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = transformer_outputs.last_hidden_state  # Shape: (batch_size, seq_len, 768)\n",
    "        x = hidden_states.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Step 5: Train the ProtBERT model separately\n",
    "def train_protbert(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs=10):\n",
    "    protbert_train_losses = []\n",
    "    protbert_val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Record the average training loss for this epoch\n",
    "        protbert_train_losses.append(train_loss / len(train_dataloader))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evalua\n",
    "\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Record the average validation loss for this epoch\n",
    "        protbert_val_losses.append(val_loss / len(val_dataloader))\n",
    "\n",
    "    return protbert_train_losses, protbert_val_losses\n",
    "\n",
    "# Step 6: Train the CNN/LSTM model\n",
    "def train_cnn_lstm(cnn_model, lstm_model, train_dataloader, val_dataloader, optimizer_cnn, optimizer_lstm, criterion, num_epochs=10):\n",
    "    cnn_train_losses = []\n",
    "    cnn_val_losses = []\n",
    "    lstm_train_losses = []\n",
    "    lstm_val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train CNN\n",
    "        cnn_model.train()  # Set CNN model to training mode\n",
    "        cnn_train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer_cnn.zero_grad()\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs_cnn = cnn_model(inputs)\n",
    "            loss_cnn = criterion(outputs_cnn, labels)\n",
    "            loss_cnn.backward()\n",
    "            optimizer_cnn.step()\n",
    "\n",
    "            cnn_train_loss += loss_cnn.item()\n",
    "\n",
    "        # Record the average CNN training loss\n",
    "        cnn_train_losses.append(cnn_train_loss / len(train_dataloader))\n",
    "\n",
    "        # Train LSTM\n",
    "        lstm_model.train()  # Set LSTM model to training mode\n",
    "        lstm_train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer_lstm.zero_grad()\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs_lstm = lstm_model(inputs)\n",
    "            loss_lstm = criterion(outputs_lstm, labels)\n",
    "            loss_lstm.backward()\n",
    "            optimizer_lstm.step()\n",
    "\n",
    "            lstm_train_loss += loss_lstm.item()\n",
    "\n",
    "        # Record the average LSTM training loss\n",
    "        lstm_train_losses.append(lstm_train_loss / len(train_dataloader))\n",
    "\n",
    "        # Validation phase for CNN\n",
    "        cnn_model.eval()\n",
    "        cnn_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs_cnn = cnn_model(inputs)\n",
    "                loss_cnn = criterion(outputs_cnn, labels)\n",
    "                cnn_val_loss += loss_cnn.item()\n",
    "\n",
    "        # Record the average CNN validation loss\n",
    "        cnn_val_losses.append(cnn_val_loss / len(val_dataloader))\n",
    "\n",
    "        # Validation phase for LSTM\n",
    "        lstm_model.eval()\n",
    "        lstm_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs_lstm = lstm_model(inputs)\n",
    "                loss_lstm = criterion(outputs_lstm, labels)\n",
    "                lstm_val_loss += loss_lstm.item()\n",
    "\n",
    "        # Record the average LSTM validation loss\n",
    "        lstm_val_losses.append(lstm_val_loss / len(val_dataloader))\n",
    "\n",
    "    return cnn_train_losses, cnn_val_losses, lstm_train_losses, lstm_val_losses\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "def evaluate_model(model, eval_dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    report = classification_report(true_labels, predictions, target_names=[\"Type A\", \"Type B\", \"Type C\"])\n",
    "    print(report)\n",
    "\n",
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(WeightedLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        loss = nn.CrossEntropyLoss(weight=self.weights)\n",
    "        return loss(outputs, targets)\n",
    "\n",
    "def augment_sequence(sequence):\n",
    "    # Introduce random mutations (simple example)\n",
    "    mutated_sequence = list(sequence)\n",
    "    for i in range(len(sequence)):\n",
    "        if random.random() < 0.1:  # 10% chance to mutate\n",
    "            mutated_sequence[i] = random.choice('ACGT')\n",
    "    return ''.join(mutated_sequence)\n",
    "\n",
    "def plot_loss_curve(protbert_losses, cnn_losses, lstm_losses,\n",
    "                    protbert_val_losses, cnn_val_losses, lstm_val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the losses for ProtBERT\n",
    "    plt.plot(protbert_losses, label='ProtBERT Training Loss', color='blue')\n",
    "    plt.plot(protbert_val_losses, label='ProtBERT Validation Loss', color='blue', linestyle='--')\n",
    "\n",
    "    # Plot the losses for CNN\n",
    "    plt.plot(cnn_losses, label='CNN Training Loss', color='green')\n",
    "    plt.plot(cnn_val_losses, label='CNN Validation Loss', color='green', linestyle='--')\n",
    "\n",
    "    # Plot the losses for LSTM\n",
    "    plt.plot(lstm_losses, label='LSTM Training Loss', color='red')\n",
    "    plt.plot(lstm_val_losses, label='LSTM Validation Loss', color='red', linestyle='--')\n",
    "    \n",
    "    plt.title('Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Assuming you have true labels and predictions for each model\n",
    "def plot_confusion_matrix(true_labels, protbert_preds, cnn_preds, lstm_preds):\n",
    "    # ProtBERT Confusion Matrix\n",
    "    protbert_cm = confusion_matrix(true_labels, protbert_preds)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(protbert_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['Class 0', 'Class 1', 'Class 2'], \n",
    "                yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "    plt.title('ProtBERT Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # CNN Confusion Matrix\n",
    "    cnn_cm = confusion_matrix(true_labels, cnn_preds)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cnn_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['Class 0', 'Class 1', 'Class 2'], \n",
    "                yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "    plt.title('CNN Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # LSTM Confusion Matrix\n",
    "    lstm_cm = confusion_matrix(true_labels, lstm_preds)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(lstm_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['Class 0', 'Class 1', 'Class 2'], \n",
    "                yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "    plt.title('LSTM Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics(true_labels, protbert_preds, cnn_preds, lstm_preds):\n",
    "    # Generate classification report\n",
    "    protbert_report = classification_report(true_labels, protbert_preds, output_dict=True)\n",
    "    cnn_report = classification_report(true_labels, cnn_preds, output_dict=True)\n",
    "    lstm_report = classification_report(true_labels, lstm_preds, output_dict=True)\n",
    "\n",
    "    # Extract metrics\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1-score']\n",
    "    protbert_scores = [protbert_report['accuracy'], protbert_report['macro avg']['precision'], \n",
    "                       protbert_report['macro avg']['recall'], protbert_report['macro avg']['f1-score']]\n",
    "    cnn_scores = [cnn_report['accuracy'], cnn_report['macro avg']['precision'], \n",
    "                  cnn_report['macro avg']['recall'], cnn_report['macro avg']['f1-score']]\n",
    "    lstm_scores = [lstm_report['accuracy'], lstm_report['macro avg']['precision'], \n",
    "                   lstm_report['macro avg']['recall'], lstm_report['macro avg']['f1-score']]\n",
    "\n",
    "    # Plot bar chart\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.2  # Bar width\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.bar(x - width, protbert_scores, width, label='ProtBERT')\n",
    "    ax.bar(x, cnn_scores, width, label='CNN')\n",
    "    ax.bar(x + width, lstm_scores, width, label='LSTM')\n",
    "\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Model Comparison: Accuracy, Precision, Recall, F1 Score')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(true_labels, protbert_preds, cnn_preds, lstm_preds):\n",
    "    fpr_protbert, tpr_protbert, _ = roc_curve(true_labels, protbert_preds)\n",
    "    fpr_cnn, tpr_cnn, _ = roc_curve(true_labels, cnn_preds)\n",
    "    fpr_lstm, tpr_lstm, _ = roc_curve(true_labels, lstm_preds)\n",
    "\n",
    "    roc_auc_protbert = auc(fpr_protbert, tpr_protbert)\n",
    "    roc_auc_cnn = auc(fpr_cnn, tpr_cnn)\n",
    "    roc_auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr_protbert, tpr_protbert, color='blue', label=f'ProtBERT AUC = {roc_auc_protbert:.2f}')\n",
    "    plt.plot(fpr_cnn, tpr_cnn, color='green', label=f'CNN AUC = {roc_auc_cnn:.2f}')\n",
    "    plt.plot(fpr_lstm, tpr_lstm, color='red', label=f'LSTM AUC = {roc_auc_lstm:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mval_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Step 8: Load and prepare data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRostlab/prot_bert_bfd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 8: Load and prepare data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "sequences, labels = read_fasta(\"your_protein_sequences.fasta\")\n",
    "\n",
    "# Split the data into training and evaluation sets\n",
    "#need to create validation sequences and labels\n",
    "train_sequences, eval_sequences, train_labels, eval_labels = train_test_split(sequences, labels, test_size=0.2)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = ProteinDataset(train_sequences, train_labels, tokenizer)\n",
    "val_dataset = ProteinDataset(val_sequences, val_labels, tokenizer)\n",
    "eval_dataset = ProteinDataset(eval_sequences, eval_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "criterion = WeightedLoss(weights=torch.tensor([1.0, 1.0, 1.0]).to(device))\n",
    "# Initialize optimizers for CNN and LSTM models\n",
    "optimizer_cnn = torch.optim.Adam(CNN_Model.parameters(), lr=1e-3)\n",
    "optimizer_lstm = torch.optim.Adam(LSTM_Model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ProtBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Initialize the ProtBERT model for fine-tuning\n",
    "transformer_model = AutoModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "protbert_model = ProtBERT_Model(transformer_model)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "protbert_model.to(device)\n",
    "\n",
    "# Step 10: Set up optimizer for ProtBERT\n",
    "optimizer = torch.optim.Adam(protbert_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Step 11: Fine-tune ProtBERT model\n",
    "train_protbert(ProtBERT_Model, train_dataloader, val_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protbert_losses = []  # List of training losses for ProtBERT\n",
    "cnn_losses = []       # List of training losses for CNN\n",
    "lstm_losses = []      # List of training losses for LSTM\n",
    "\n",
    "# Similarly, track validation losses\n",
    "protbert_val_losses = []\n",
    "cnn_val_losses = []\n",
    "lstm_val_losses = []\n",
    "\n",
    "# Assuming `test_labels` are the true labels for the test set:\n",
    "protbert_preds = protbert_model.predict(eval_dataset)\n",
    "cnn_preds = CNN_Model.predict(eval_dataset)\n",
    "lstm_preds = LSTM_Model.predict(eval_dataset)\n",
    "\n",
    "# True labels for the test set (ensure this matches the order of your data)\n",
    "true_labels = eval_labels\n",
    "\n",
    "# Assuming `cnn_model` and `lstm_model` are already defined, and `train_dataloader` and `val_dataloader` are prepared\n",
    "# Also assuming optimizers for both CNN and LSTM models are defined (`optimizer_cnn` and `optimizer_lstm`)\n",
    "\n",
    "cnn_train_losses, cnn_val_losses, lstm_train_losses, lstm_val_losses = train_cnn_lstm(\n",
    "    CNN_Model, LSTM_Model, train_dataloader, val_dataloader, optimizer_cnn, optimizer_lstm, criterion, num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = 'cnn' \n",
    "\n",
    "if model_choice == 'cnn':\n",
    "    cnn_model = CNN_Model()\n",
    "    cnn_model.train() # Set model to training mode\n",
    "    cnn_model.to(device)\n",
    "    optimizer = torch.optim.Adam(cnn_model.parameters(), lr=1e-5)\n",
    "    train_cnn_lstm(cnn_model, train_dataloader, optimizer, device, num_epochs=3)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "cnn_model.eval()\n",
    "\n",
    "evaluate_model(cnn_model if model_choice == 'cnn' else LSTM_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = 'LSTM' \n",
    "\n",
    "if model_choice == 'LSTM':\n",
    "    lstm_model = LSTM_Model()\n",
    "    lstm_model.train() # Set model to training mode\n",
    "    lstm_model.to(device)\n",
    "    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-5)\n",
    "    train_cnn_lstm(lstm_model, train_dataloader, optimizer, device, num_epochs=3)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "lstm_model.eval()\n",
    "\n",
    "# Step 13: Evaluate the final model (CNN or LSTM)\n",
    "evaluate_model(cnn_model if model_choice == 'cnn' else lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the loss vs epoch plot function\n",
    "plot_loss_curve(protbert_losses, cnn_losses, lstm_losses,\n",
    "                protbert_val_losses, cnn_val_losses, lstm_val_losses)\n",
    "\n",
    "# Call the confusion matrix function\n",
    "plot_confusion_matrix(true_labels, protbert_preds, cnn_preds, lstm_preds)\n",
    "\n",
    "# Call the metrics comparison function\n",
    "plot_metrics(true_labels, protbert_preds, cnn_preds, lstm_preds)\n",
    "\n",
    "# Calculate ROC curve and AUC during validation\n",
    "true_labels = []  # To store the true labels\n",
    "preds = []  # To store the predicted scores\n",
    "\n",
    "cnn_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn_model(inputs)\n",
    "\n",
    "        # Store true labels and predicted scores\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "# Calculate ROC and AUC\n",
    "fpr, tpr, _ = roc_curve(true_labels, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n",
    "lstm_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = lstm_model(inputs)\n",
    "\n",
    "        # Store true labels and predicted scores\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "# Calculate ROC and AUC\n",
    "fpr, tpr, _ = roc_curve(true_labels, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n",
    "# Call the ROC curve function\n",
    "plot_roc_curve(true_labels, protbert_preds, cnn_preds, lstm_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
